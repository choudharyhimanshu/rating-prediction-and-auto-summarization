{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "\n",
    "from gensim import summarization\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from math import floor,ceil\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rating_to_polarity(rating):\n",
    "    if rating > 3:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def score_to_polarity(score):\n",
    "    if score < 0:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def score_to_rating(score):\n",
    "    if score < -.7:\n",
    "        return 1\n",
    "    if score < -.2:\n",
    "        return 2\n",
    "    if score < .2:\n",
    "        return 3\n",
    "    if score < .7:\n",
    "        return 4\n",
    "    return 5\n",
    "#     rating = score*2+3\n",
    "#     return int(round(rating))\n",
    "\n",
    "def get_weight(rating):\n",
    "    if (rating[5] == 1):\n",
    "        return 2\n",
    "    if (rating[1] == 1):\n",
    "        return 1\n",
    "    return 1\n",
    "\n",
    "def get_keywords(text):\n",
    "    try:\n",
    "        keywords = summarization.keywords(text,ratio=1.0,split=True)\n",
    "    except Exception:\n",
    "        keywords = []\n",
    "    return ' '.join(keywords)\n",
    "\n",
    "def categorize(ratings):\n",
    "    cats = []\n",
    "    for rating in ratings:\n",
    "        v = [0,0,0,0,0]\n",
    "        v[rating-1] = 1\n",
    "        cats.append(v)\n",
    "    return np.array(cats)\n",
    "\n",
    "def generate_random_rating():\n",
    "    a = np.random.randint(low=1,high=6,size=1)\n",
    "    return np.mean(a,dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/Reviews_uniform_25000.csv',header=0,index_col=0,encoding='utf-8')\n",
    "data = data.sample(n=5000,random_state=1)\n",
    "# data = data[data.Score != 3]\n",
    "data = data.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summaries = data.Summary\n",
    "summaries = data.Text\n",
    "# summaries = data.Text.map(get_keywords)\n",
    "ratings = data.Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=.8)\n",
    "vectorizer.fit(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = vectorizer.transform(summaries).toarray()\n",
    "# y = ((ratings-3)/2.0).values    # for polarity score\n",
    "# y = ratings.map(rating_to_polarity).values      # for polarity classification\n",
    "y = categorize(ratings.values)   # for rating classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2)\n",
    "# w_train = np.array(map(get_weight,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128,input_dim=X_train.shape[1]))\n",
    "\n",
    "# model.add(Dense(1,activation='sigmoid'))         # for polarity classification\n",
    "# model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "\n",
    "model.add(Dense(5,activation='softmax'))         # for rating classification\n",
    "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "\n",
    "# model.add(Dense(1,activation='tanh'))         # for polarity score\n",
    "# model.compile(loss='mean_squared_error',optimizer='rmsprop',metrics=['mean_squared_error'])\n",
    "\n",
    "model.fit(X_train,y_train,nb_epoch=10,batch_size=32,verbose=1)\n",
    "# model.fit(X_train,y_train,sample_weight=w_train,nb_epoch=30,batch_size=32,verbose=1)\n",
    "model.evaluate(X_test,y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "out = []\n",
    "for i in range(len(preds)):\n",
    "#     out.append([score_to_rating(preds[i][0]),int(y_test[i]*2+3)])     # for polarity score\n",
    "#     out.append([int(round(preds[i][0])),y_test[i]])     # for polarity classification\n",
    "    out.append([preds[i].argmax()+1,y_test[i].argmax()+1])    # for rating classification\n",
    "\n",
    "out = pd.DataFrame(out,columns=['PredictedRating','ActualRating'])\n",
    "out['RandomRating'] = pd.Series([generate_random_rating() for _ in range(len(out))])\n",
    "out['DiffActPred'] = (out.ActualRating - out.PredictedRating).map(abs)\n",
    "out['DiffActRand'] = (out.ActualRating - out.RandomRating).map(abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out[['ActualRating','PredictedRating','RandomRating']].hist()\n",
    "out[['DiffActPred','DiffActRand']].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Dataset size : {:d}\".format(len(data))\n",
    "print \"Training set size : {:d}\".format(len(X_train))\n",
    "print \"Testing set size : {:d}\".format(len(X_test))\n",
    "print \"Accuracy between predicted and actual : {:f}\".format(accuracy_score(out.PredictedRating,out.ActualRating))\n",
    "print \"Accuracy between random and actual : {:f}\".format(accuracy_score(out.RandomRating,out.ActualRating))\n",
    "print \"Accuracy with +-1 difference between predicted and actual : {:f}\".format(float(out.DiffActPred.value_counts()[0]+out.DiffActPred.value_counts()[1])/len(out))\n",
    "print \"Accuracy with +-1 difference between random and actual : {:f}\".format(float(out.DiffActRand.value_counts()[0]+out.DiffActRand.value_counts()[1])/len(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out.to_csv('output/rating_gen_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
